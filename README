DRFCSharp
-------------------------------------

A C# library to train and infer with Discriminative Random Fields.

Prerequisites
--------------

DRFCSharp was coded in Monodevelop, and uses .NET 4.0 language
features. To run it, you will need to make sure you have:

.NET Framework 4.0
MathNet.Numerics http://numerics.mathdotnet.com/
NUnit (for unit tests)

Subdirectories
--------------

    /DataLabeler/
    --------------
    
    Contains the Python 2.7 script we use to view and edit labelings;
    it also provides statistics on the accuracy of predicted
    classifications.
    
    /Dataset/
    --------------
    
    Contains our Maryland GIS dataset. Make sure to pass --xsites 16
    to the program if you run it on this dataset.
    
    Also contains all of the XML files that contain stored model
    parameters.
    
    /DatasetKH/
    --------------
    
    Contains the dataset from Kumar and Hebert (2006). There are 24 x
    sites in these images, but the program defaults to 24 x sites.
    
    /DRFCSharp/
    --------------
    
    Contains the actual C# code, and the compiled executables.
    
    /EdgeWeightDerivation/
    --------------
    
    Contains LaTeX code for our derivation of the log-likelihood ratio
    of a classification at a single site, for use in transforming the
    problem to a graph. It turned out this was not useful for
    transforming the problem to a graph.
    
    /GradientDerivation/
    --------------
    
    Contains LaTeX code for our derivation of the gradient of the
    log-likelihood of the parameters given a dataset. We use these
    equations to gradient ascend to train the model.
    
    /KumarAndHebertLabelConverter/
    --------------
    
    Contains a Python script, which we ran once, to convert Kumar and
    Hebert's data into our csv-based format.
    
    /Montgomery County Government GIS Internet Map Disclaimer/
    --------------
    
    Contains the disclaimer for our dataset.
    
    /Papers/
    --------------
    
    Contains the pdf copies of the papers we consulted in working on
    this project.