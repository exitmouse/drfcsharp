Our attempt to replicate most closely the hyperparameter choice of
Kumar and Hebert gave results that were nowhere near comparable to
theirs. They were, however, significantly better than random
chance. We used ICM inference, rather than MAP inference, for the
reasons we mention, and in aggregate we had a detection rate of 34%
with an average of 83 false positives per image. Random chance would
give 115. Out of 91008 sites, we predicted 23,674 as on, 19,683 of
which were false positives; the number of sites actually on was
11,575. The classifications showed wide variation; some were accurate
and interpretable, while others labeled vast swathes of the images as
on for seemingly no reason. We speculate that this behavior was
partially due to the choice of ICM as an inference mechanism, but if
we compare our logistic classifications to theirs, we see that the
problem must really lie with our features. They achieved much better
results with just a logistic classification than we were able to;
either we are training our model wrong, or our features are just not
usefully correlated with being a manmade structure. The results we
have gotten when removing the moment-based features, and the
dependencies we have observed of our model on the moment-based
features both indicate that our right-angle detection and
orientation-based inter-scale features are effectively useless.
